{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 198,
     "status": "ok",
     "timestamp": 1753166171704,
     "user": {
      "displayName": "Anwesh Nayak",
      "userId": "10812060260948452341"
     },
     "user_tz": -330
    },
    "id": "zW7Yu_N27D4V",
    "outputId": "e2c9cc26-dae3-4b1f-d3dc-0a78f1dac33e"
   },
   "outputs": [],
   "source": [
    "%env CUBLAS_WORKSPACE_CONFIG=:4096:8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5InbErDf38LF"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchinfo import summary\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "import glob\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader,Subset\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,confusion_matrix\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bKJQJzRDP4yr"
   },
   "outputs": [],
   "source": [
    "def set_seed(seed: int):\n",
    "    \"\"\"Seed everything for reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    # enforce deterministic algorithms (may slow things down)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # torch 2.x: fully deterministic\n",
    "    if hasattr(torch, \"use_deterministic_algorithms\"):\n",
    "        torch.use_deterministic_algorithms(True)\n",
    "# os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "# choose your seed\n",
    "seed_list = [3,5,11,1344,2506]\n",
    "SEED = 3\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1753166192918,
     "user": {
      "displayName": "Anwesh Nayak",
      "userId": "10812060260948452341"
     },
     "user_tz": -330
    },
    "id": "dQqv_b4U3_vp",
    "outputId": "77f35a17-2ff7-49f5-b98a-e42e478dc49e"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train_df_encoded1.csv')\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1753166210186,
     "user": {
      "displayName": "Anwesh Nayak",
      "userId": "10812060260948452341"
     },
     "user_tz": -330
    },
    "id": "4AOFu8iMjVTn",
    "outputId": "c04efa26-3c39-417b-8651-aa29f176ac8b"
   },
   "outputs": [],
   "source": [
    "total_abnormal_count = (train_df['label'] == 1).sum() \n",
    "total_normal_count = (train_df['label'] == 0).sum()\n",
    "total_samples = len(train_df)\n",
    "\n",
    "# Print\n",
    "print(\"Total abnormal images:\", total_abnormal_count)\n",
    "print(\"Total normal images:\", total_normal_count)\n",
    "print(\"Total samples:\", total_samples)\n",
    "# Inverse frequency\n",
    "weight_normal = 1 / total_normal_count\n",
    "weight_abnormal = 1 / total_abnormal_count\n",
    "\n",
    "# Normalize\n",
    "total_inv = weight_normal + weight_abnormal\n",
    "weight_normal /= total_inv\n",
    "weight_abnormal /= total_inv\n",
    "\n",
    "# PyTorch tensor\n",
    "class_weights = torch.tensor([weight_normal, weight_abnormal], dtype=torch.float32).to(device)\n",
    "print(class_weights)\n",
    "print('Weight Ratio:',class_weights[1]/class_weights[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NxA_lSAFTJIx"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to fit most CNNs\n",
    "    transforms.ToTensor(),          # Convert to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # ImageNet mean/std\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.df.loc[idx, 'image_path']\n",
    "        label = self.df.loc[idx, 'label']\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "\n",
    "train_dataset = ImageDataset(train_df, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BHLmwM8PYNdq"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1IYzb9SdJH8P"
   },
   "outputs": [],
   "source": [
    "# --- Training for one epoch -----------------------------------\n",
    "def train_epoch(model, loader, optimizer,criterion, device):\n",
    "    \n",
    "    model.train()\n",
    "    total_loss, total_correct, total_samples = 0.0, 0, 0\n",
    "    \n",
    "    step_times = []\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    for images, labels in tqdm(loader, desc='Train', leave=False):\n",
    "        step_start = time.time()\n",
    "        images, labels = images.to(device), labels.to(device).long()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images).float()                  # [batch, 2] raw logits\n",
    "        loss = criterion(outputs, labels)        # CrossEntropyLoss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        total_correct += (preds == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "        step_end = time.time()\n",
    "        step_times.append(step_end - step_start)\n",
    "        \n",
    "    avg_loss = total_loss / total_samples\n",
    "    avg_acc  = total_correct / total_samples\n",
    "\n",
    "    epoch_end = time.time()\n",
    "    epoch_time = epoch_end - epoch_start\n",
    "    \n",
    "    return avg_loss, avg_acc, epoch_time, step_times\n",
    "\n",
    "# --- Validation (no threshold sweep) --------------------------\n",
    "def validate_epoch(model, loader,criterion, device):\n",
    "    model.eval()\n",
    "    total_loss, total_correct, total_samples = 0.0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader, desc='Val', leave=False):\n",
    "            images, labels = images.to(device), labels.to(device).long()\n",
    "            outputs = model(images).float()\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            total_correct += (preds == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "    avg_acc  = total_correct / total_samples\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "\n",
    "def plot_kfold_metrics(metrics_path, model_name=None, save_dir=None, dpi=500):\n",
    "    # --- Load JSON ---\n",
    "    with open(metrics_path, \"r\") as f:\n",
    "        metrics_data = json.load(f)\n",
    "\n",
    "    per_fold_data = metrics_data[\"per_fold\"]\n",
    "    num_folds = len(per_fold_data)\n",
    "\n",
    "    # --- Inference ---\n",
    "    if model_name is None:\n",
    "        model_name = os.path.splitext(os.path.basename(metrics_path))[0].replace(\"_training_metrics_all_folds\", \"\")\n",
    "\n",
    "    if save_dir is None:\n",
    "        save_dir = os.path.dirname(metrics_path)\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # --- Plot Each Fold ---\n",
    "    for fold_idx, fold_metrics in enumerate(per_fold_data, 1):\n",
    "        train_loss = fold_metrics[\"train_loss\"]\n",
    "        val_loss = fold_metrics[\"val_loss\"]\n",
    "        train_acc = fold_metrics[\"train_acc\"]\n",
    "        val_acc = fold_metrics[\"val_acc\"]\n",
    "        epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "        fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "        # Loss\n",
    "        l1 = ax1.plot(epochs, train_loss, label='Train Loss', marker='o', color='tab:red')\n",
    "        l2 = ax1.plot(epochs, val_loss, label='Validation Loss', marker='o', linestyle='--', color='tab:orange')\n",
    "        ax1.set_xlabel('Epochs')\n",
    "        ax1.set_ylabel('Loss', color='tab:red')\n",
    "        ax1.tick_params(axis='y', labelcolor='tab:red')\n",
    "\n",
    "        # Accuracy\n",
    "        ax2 = ax1.twinx()\n",
    "        l3 = ax2.plot(epochs, train_acc, label='Train Accuracy', marker='o', color='tab:blue')\n",
    "        l4 = ax2.plot(epochs, val_acc, label='Validation Accuracy', marker='o', linestyle='--', color='tab:cyan')\n",
    "        ax2.set_ylabel('Accuracy', color='tab:blue')\n",
    "        ax2.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "        # Combine legends\n",
    "        lines = l1 + l2 + l3 + l4\n",
    "        labels = [line.get_label() for line in lines]\n",
    "        fig.legend(\n",
    "            handles=lines,\n",
    "            labels=labels,\n",
    "            loc='upper center',\n",
    "            bbox_to_anchor=(0.5, 1.15),\n",
    "            ncol=2,\n",
    "            fontsize='medium',\n",
    "            frameon=True\n",
    "        )\n",
    "\n",
    "        # Title and layout\n",
    "        plt.title(f'{model_name} - Fold {fold_idx}: Loss and Accuracy over Epochs')\n",
    "        plt.subplots_adjust(top=0.82, bottom=0.1)\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save\n",
    "        save_path = os.path.join(save_dir, f\"{model_name}_fold{fold_idx}_metrics.png\")\n",
    "        plt.savefig(save_path, dpi=dpi, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        print(f\"✅ Saved plot: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_by_name(name: str, num_classes=2, device='cuda'):\n",
    "    base_model = models.resnet18\n",
    "    # --- Pretrained or not ---\n",
    "    if \"scratch\" in name:\n",
    "        model = base_model(weights=None, zero_init_residual=True)\n",
    "    else:\n",
    "        model = base_model(pretrained=True)\n",
    "    # --- Modifications ---\n",
    "    in_features = model.fc.in_features\n",
    "    dropout_match = re.search(r'dp\\(([\\d.]+)\\)', name)\n",
    "    dropout_p = float(dropout_match.group(1)) if dropout_match else None\n",
    "    \n",
    "    if \"mod2\" in name:\n",
    "        # Mod 2: Two-layer MLP with dropout\n",
    "        model.fc = nn.Sequential(\n",
    "            nn.Linear(in_features, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout_p if dropout_p is not None else 0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in model.layer4[:].parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in model.fc.parameters():\n",
    "            param.requires_grad = True\n",
    "            \n",
    "    elif \"mod1\" in name:\n",
    "        # Mod 1: Dropout + final layer\n",
    "        model.fc = nn.Sequential(\n",
    "            nn.Linear(in_features, in_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout_p if dropout_p is not None else 0.5),\n",
    "            nn.Linear(in_features, num_classes)\n",
    "                                )\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in model.layer4[:].parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in model.fc.parameters():\n",
    "            param.requires_grad = True\n",
    " \n",
    "    else:\n",
    "        # Default\n",
    "        model.fc = nn.Linear(in_features, num_classes)\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in model.layer4[:].parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in model.fc.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    return model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 32\n",
    "NUM_FOLDS     = 5\n",
    "BATCH_SIZE    = 32\n",
    "EARLY_STOPING_PATIENCE = 20\n",
    "INITIAL_LR = 0.001\n",
    "REDUCE_LR_PATIENCE = 10\n",
    "REDUCE_LR_FACTOR = 0.5\n",
    "CLASS_WEIGHTS = class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_kfold_training(\n",
    "    model_name_prefix,\n",
    "    train_dataset = train_dataset,\n",
    "    get_model_fn = get_model_by_name,\n",
    "    train_epoch_fn = train_epoch,\n",
    "    validate_epoch_fn = validate_epoch,\n",
    "    plot_metrics_fn = plot_kfold_metrics,\n",
    "    class_weights = CLASS_WEIGHTS,\n",
    "    device='cuda',\n",
    "    num_folds=NUM_FOLDS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    patience=EARLY_STOPING_PATIENCE,\n",
    "    lr=INITIAL_LR,\n",
    "    weight_decay=1e-5,\n",
    "    seed=SEED,\n",
    "    dir_path = os.getcwd(),\n",
    "    metrics_filename=None\n",
    "):\n",
    "\n",
    "    model_dir = os.path.join(dir_path, model_name_prefix)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=seed)\n",
    "    all_fold_metrics = []\n",
    "    print('Training',model_name_prefix) \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(train_dataset), 1):\n",
    "        print(f\"\\n=== Fold {fold}/{num_folds} ===\")\n",
    "\n",
    "        # Prepare loaders\n",
    "        train_loader = DataLoader(Subset(train_dataset, train_idx), batch_size=batch_size, shuffle=True)\n",
    "        val_loader   = DataLoader(Subset(train_dataset, val_idx), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # Model and optimizer\n",
    "        model = get_model_fn(model_name_prefix)\n",
    "        optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=weight_decay)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=REDUCE_LR_PATIENCE)\n",
    "        if 'weighted' in model_name_prefix:\n",
    "            criterion = nn.CrossEntropyLoss(weight=class_weights.to(torch.float32))\n",
    "            print('Using Weighted Loss')\n",
    "        else:\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "        # History for this fold\n",
    "        fold_hist = {\n",
    "            \"train_loss\": [], \"train_acc\": [],\n",
    "            \"val_loss\": [],   \"val_acc\": [],\n",
    "            \"epoch_time_s\": [], \"avg_step_time_s\": [],\n",
    "            \"lr_history\": []\n",
    "        }\n",
    "\n",
    "        best_loss = float('inf')\n",
    "        epochs_no_improve = 0\n",
    "\n",
    "        model_basename = f\"{model_name_prefix}_fold{fold}\"\n",
    "        model_path = os.path.join(model_dir, f\"{model_basename}.pth\")\n",
    "        best_model_path = os.path.join(model_dir, f\"{model_basename}_best.pth\")\n",
    "\n",
    "        for epoch in range(1, num_epochs + 1):\n",
    "            train_loss, train_acc, epoch_time, step_times = train_epoch_fn(model, train_loader, optimizer, criterion, device)\n",
    "            val_loss, val_acc = validate_epoch_fn(model, val_loader, criterion, device)\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "            # Record\n",
    "            fold_hist[\"train_loss\"].append(train_loss)\n",
    "            fold_hist[\"train_acc\"].append(train_acc)\n",
    "            fold_hist[\"val_loss\"].append(val_loss)\n",
    "            fold_hist[\"val_acc\"].append(val_acc)\n",
    "            fold_hist[\"epoch_time_s\"].append(epoch_time)\n",
    "            fold_hist[\"avg_step_time_s\"].append(sum(step_times) / len(step_times))\n",
    "            fold_hist[\"lr_history\"].append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "            print(f\"Epoch {epoch:02d}: Train Loss={train_loss:.4f}, Acc={train_acc*100:.2f}% | \"\n",
    "                  f\"Val Loss={val_loss:.4f}, Acc={val_acc*100:.2f}% | LR={fold_hist['lr_history'][-1]:.6f}\")\n",
    "\n",
    "            if val_loss < best_loss:\n",
    "                best_loss = val_loss\n",
    "                torch.save(model.state_dict(), best_model_path)\n",
    "                epochs_no_improve = 0\n",
    "                print(\"  → New best model saved\")\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "                if epochs_no_improve >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch}\")\n",
    "                    break\n",
    "\n",
    "        # Save final model for fold\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        all_fold_metrics.append(fold_hist)\n",
    "\n",
    "    # Aggregate validation loss\n",
    "    def agg(key):\n",
    "        arr = [np.array(f[key]) for f in all_fold_metrics]\n",
    "        min_len = min(len(a) for a in arr)\n",
    "        stacked = np.stack([a[:min_len] for a in arr], axis=0)\n",
    "        return stacked.mean(axis=0), stacked.std(axis=0)\n",
    "\n",
    "    mean_val_loss, std_val_loss = agg(\"val_loss\")\n",
    "\n",
    "    \n",
    "    # Print summary — Last 5 epochs\n",
    "    print(\"\\n=== Cross‐Val Summary (last 5 epochs) ===\")\n",
    "    num_epochs = len(mean_val_loss)\n",
    "    start_epoch = max(0, num_epochs - 5)\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        print(f\"Epoch {epoch + 1:02d}: Val Loss = {mean_val_loss[epoch]:.4f} ± {std_val_loss[epoch]:.4f}\")\n",
    "\n",
    "    # Save to file\n",
    "    if metrics_filename is None:\n",
    "        metrics_filename = f\"{model_name_prefix}_training_metrics_all_folds.json\"\n",
    "    metrics_path = os.path.join(model_dir, metrics_filename)\n",
    "\n",
    "    with open(metrics_path, \"w\") as f:\n",
    "        json.dump({\n",
    "            \"per_fold\": all_fold_metrics,\n",
    "            \"val_loss_mean\": mean_val_loss.tolist(),\n",
    "            \"val_loss_std\": std_val_loss.tolist()\n",
    "        }, f, indent=4)\n",
    "    \n",
    "    print(f\"\\nK-fold cross-validation complete. Metrics written to {metrics_path}.\")\n",
    "    plot_kfold_metrics(metrics_path=metrics_path)\n",
    "    model_summary_str = str(summary(model, input_size=(32, 3, 224, 224)))\n",
    "    summary_path = os.path.join(model_dir, f\"{model_name_prefix}_summary.txt\")\n",
    "    with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(model_summary_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [\n",
    "    \"ResNet18_scratch\",\n",
    "    \"ResNet18_weighted_scratch\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in model_list:\n",
    "    run_kfold_training(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyM5OT7HPsGvtKPAH4bpQK9F",
   "gpuType": "A100",
   "machine_shape": "hm",
   "mount_file_id": "1LLELlEvQbOLqf5l69y1Fu2yOTihqRxb9",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
